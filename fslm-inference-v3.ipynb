{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pavanbagli/fslm-inference-v3?scriptVersionId=177061704\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Install and Imports","metadata":{"id":"5ZXjh6D9nOUX"}},{"cell_type":"code","source":"import os\nimport sys\nimport httplib2\nimport pyparsing\nimport apache_beam as beam\n\nprint('httplib2 version: {}'.format(httplib2.__version__))\nprint('pyparsing version: {}'.format(pyparsing.__version__))\nprint('beam version: {}'.format(beam.__version__))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system('pip3 install --no-index --find-links ../input/tftext/tensorflowtext2600wheel/kaggle/working tensorflow-text==2.6.0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom typing import Dict, Text, Any, Tuple, List\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nimport tensorflow as tf\nimport tensorflow_text\nfrom tensorflow import keras\nfrom tensorflow.core.example import example_pb2\n\nimport tfx_bsl\nfrom tfx_bsl.public.beam import RunInference\nfrom tfx_bsl.public import tfxio\nfrom tfx_bsl.public.proto import model_spec_pb2\n\ntf.get_logger().setLevel('ERROR')\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('Beam version: {}'.format(beam.__version__))","metadata":{"id":"TK6SyLhQP_s5","outputId":"a8ee43fd-9479-4da2-a76e-56436d23517b","execution":{"iopub.status.busy":"2022-07-05T19:28:34.377371Z","iopub.execute_input":"2022-07-05T19:28:34.37776Z","iopub.status.idle":"2022-07-05T19:28:40.505623Z","shell.execute_reply.started":"2022-07-05T19:28:34.377727Z","shell.execute_reply":"2022-07-05T19:28:40.503006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL_DIR = '../input/fslm-basemodel-v3/model/6/Format-Serving'\n\nMODEL_DIR = '../input/fslm-basemodel-v5/pipeline/Trainer/model/6/Format-Serving'\n\nINPUT_SOURCE_DIR = '../input/foursquare-location-matching'\nINPUT_SOURCE_TEST_DATA = f'{INPUT_SOURCE_DIR}/test.csv'\nINPUT_SOURCE_TRAIN_DATA = f'{INPUT_SOURCE_DIR}/train.csv'\n\nSERVING_DIR = './serving'\nSERVING_DATA = f'{SERVING_DIR}/serve.csv'\n\nTFRECORD_DIR = './tfrecords' \nOUTPUT_RESULTS = './output/'\nSUBMISSION_CSV = './submission.csv'\n\n!mkdir -p {SERVING_DIR}\n!mkdir -p {TFRECORD_DIR}\n!mkdir -p {OUTPUT_RESULTS}","metadata":{"id":"TBJqC6-HfymZ","execution":{"iopub.status.busy":"2022-07-05T19:28:45.940359Z","iopub.execute_input":"2022-07-05T19:28:45.940799Z","iopub.status.idle":"2022-07-05T19:28:48.070578Z","shell.execute_reply.started":"2022-07-05T19:28:45.940766Z","shell.execute_reply":"2022-07-05T19:28:48.069232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Load Saved Model","metadata":{"id":"bsHcencfobKL"}},{"cell_type":"code","source":"model = tf.keras.models.load_model(MODEL_DIR)","metadata":{"id":"vXTh751Hb_MO","outputId":"09a5d08a-050a-4634-e893-adab57b4ecb7","execution":{"iopub.status.busy":"2022-07-05T19:29:00.832576Z","iopub.execute_input":"2022-07-05T19:29:00.832978Z","iopub.status.idle":"2022-07-05T19:29:17.490681Z","shell.execute_reply.started":"2022-07-05T19:29:00.832941Z","shell.execute_reply":"2022-07-05T19:29:17.489418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Load Data for Inference","metadata":{"id":"nhiXA2xTZX2q"}},{"cell_type":"code","source":"def generate_pairs(df_1, df_2, SERVING_FILE_PATH):\n    pairs_temp=df_1.merge(df_2, how='cross', suffixes=('_1', '_2'))\n    pairs_temp = pairs_temp[pairs_temp['id_1'] != pairs_temp['id_2']]\n    \n    #combine address columns : address_1, city_1, state_1, zip_1 to full_address_1\n    pairs_temp['full_address_1'] = pairs_temp['address_1'].map(str) + \" \" + pairs_temp['city_1'].map(str) + \" \" + pairs_temp['state_1'].map(str) + \" \" + pairs_temp['zip_1'].map(str) + \" Phone: \" + pairs_temp['phone_1'].map(str)\n    pairs_temp['full_address_2'] = pairs_temp['address_2'].map(str) + \" \" + pairs_temp['city_2'].map(str) + \" \" + pairs_temp['state_2'].map(str) + \" \" + pairs_temp['zip_2'].map(str) + \" Phone: \" + pairs_temp['phone_2'].map(str)\n    pairs_temp = pairs_temp.drop(columns=['address_1','city_1','state_1','zip_1', 'phone_1', 'address_2','city_2','state_2','zip_2', 'phone_2'], axis=1)\n    pairs_temp.to_csv(SERVING_FILE_PATH, index=False)\n    del pairs_temp","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:29:21.525405Z","iopub.execute_input":"2022-07-05T19:29:21.525761Z","iopub.status.idle":"2022-07-05T19:29:21.536067Z","shell.execute_reply.started":"2022-07-05T19:29:21.525731Z","shell.execute_reply":"2022-07-05T19:29:21.534926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def csv_to_tfrecord(csv_file, tfrecord_file):\n    ''' Converts a csv file into a tfrecord\n    Args:\n    csv_file (string) - file to convert to tfrecord\n    tfrecord_file (string) - filename of tfrecord to create\n\n    Returns:\n    filename of tfrecord\n    '''\n    float_features = ['latitude_1','latitude_2','longitude_1','longitude_2']\n    byte_features = ['categories_1','categories_2','country_1','country_2','full_address_1','full_address_2','id_1','id_2','name_1','name_2','url_1','url_2']\n\n    # Open CSV file for reading. Each row is mapped as a dictionary.\n    reader = csv.DictReader(open(csv_file, 'r'))\n  \n    # Initialize TF examples list\n    examples = []\n\n    # For each row in CSV, create a TF Example based on\n    # the Schema and append to the list\n    for line in reader:\n    # Intialize example\n        example = example_pb2.Example()\n\n        for feature in float_features:\n            key=feature\n            example.features.feature[key].float_list.value[:] = (\n                [float(line[key])] if len(line[key]) > 0 else [])\n\n        for feature in byte_features:\n            key=feature\n            example.features.feature[key].bytes_list.value[:] = (\n                [line[key].encode('utf8')] if len(line[key]) > 0 else [])\n\n        # Append to the list\n        examples.append(example)\n\n    # Write examples to tfrecord file\n    with tf.io.TFRecordWriter(tfrecord_file) as writer:\n        for example in examples:\n            writer.write(example.SerializeToString())\n  \n    return tfrecord_file","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:29:50.601608Z","iopub.execute_input":"2022-07-05T19:29:50.60201Z","iopub.status.idle":"2022-07-05T19:29:50.612826Z","shell.execute_reply.started":"2022-07-05T19:29:50.601977Z","shell.execute_reply":"2022-07-05T19:29:50.611277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToCsvFn(beam.DoFn):\n    def process(self, element):\n        # This parses out the example features that were passed in during prediction\n        example = tf.io.parse_single_example(tf.make_ndarray(element.predict_log.request.inputs['examples'])[0], features)\n        example_values = [ v.numpy() for v in example.values() ]\n        example_values = [ x.decode('utf-8') if isinstance(x, bytes) else x for x in example_values ]\n        predictions = tf.make_ndarray(element.predict_log.response.outputs['outputs'])\n        full_result = example_values + list(predictions[0])\n        yield ','.join([ str(f) for f in full_result])","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:00.429366Z","iopub.execute_input":"2022-07-05T19:30:00.429751Z","iopub.status.idle":"2022-07-05T19:30:00.437836Z","shell.execute_reply.started":"2022-07-05T19:30:00.429719Z","shell.execute_reply":"2022-07-05T19:30:00.436582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictions(TFRECORD_FILE_PATH, PREDICT_FILE_PATH):\n    pipeline = beam.Pipeline()\n    tfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=TFRECORD_FILE_PATH)\n\n    with pipeline as p:\n        _ = (p | tfexample_beam_record.RawRecordBeamSource()\n               | RunInference(\n                    model_spec_pb2.InferenceSpecType(\n                    saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=MODEL_DIR)))\n               | beam.ParDo(ToCsvFn())\n               | beam.io.WriteToText(PREDICT_FILE_PATH,header='id_1,id_2,match')\n               | beam.Map(print)\n        )\n","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:03.853749Z","iopub.execute_input":"2022-07-05T19:30:03.854339Z","iopub.status.idle":"2022-07-05T19:30:03.860882Z","shell.execute_reply.started":"2022-07-05T19:30:03.854304Z","shell.execute_reply":"2022-07-05T19:30:03.859955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_output_dict(rows):\n    output = {}\n    match_ids = []\n    row_id_prev = 'Null'\n    for row in rows:\n        if row_id_prev == 'Null':\n            row_id_prev = row[0]\n            match_ids.append(row[0])\n            \n        if row[0] == row_id_prev:\n            \n            if row[2] > 0.999:\n                match_ids.append(row[1])\n        \n        else:\n            \n            output[row_id_prev] = ' '.join(match_ids)\n            match_ids = []\n            match_ids.append(row[0])\n            if row[2] > 0.999:\n                match_ids.append(row[1])\n        \n        row_id_prev = row[0]\n        \n    output[row_id_prev] = ' '.join(match_ids)\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:15.28139Z","iopub.execute_input":"2022-07-05T19:30:15.281762Z","iopub.status.idle":"2022-07-05T19:30:15.291991Z","shell.execute_reply.started":"2022-07-05T19:30:15.28173Z","shell.execute_reply":"2022-07-05T19:30:15.290948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_csv_file(output_dict):\n    with open(SUBMISSION_CSV, 'a') as csv_file:\n        writer = csv.writer(csv_file)\n        for key, value in output_dict.items():\n            writer.writerow([key, value])","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:23.299114Z","iopub.execute_input":"2022-07-05T19:30:23.29978Z","iopub.status.idle":"2022-07-05T19:30:23.31602Z","shell.execute_reply.started":"2022-07-05T19:30:23.29973Z","shell.execute_reply":"2022-07-05T19:30:23.314793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(SUBMISSION_CSV, 'w') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['id', 'matches'])","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:26.848407Z","iopub.execute_input":"2022-07-05T19:30:26.84935Z","iopub.status.idle":"2022-07-05T19:30:26.858175Z","shell.execute_reply.started":"2022-07-05T19:30:26.849304Z","shell.execute_reply":"2022-07-05T19:30:26.85704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inf_df = pd.read_csv(INPUT_SOURCE_TEST_DATA)\ninf_df.drop_duplicates(inplace=True)\nfeatures = {\n    'id_1': tf.io.FixedLenFeature([], tf.string),\n    'id_2': tf.io.FixedLenFeature([], tf.string),\n     }","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:38.265606Z","iopub.execute_input":"2022-07-05T19:30:38.265982Z","iopub.status.idle":"2022-07-05T19:30:38.298688Z","shell.execute_reply.started":"2022-07-05T19:30:38.26595Z","shell.execute_reply":"2022-07-05T19:30:38.297715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,inf_df.shape[0],100):\n    SERVING_FILE_PATH = f\"{SERVING_DIR}/{str(i) + '.csv'}\"\n    TFRECORD_FILE_PATH = f\"{TFRECORD_DIR}/{str(i) + '.tfrecord'}\"\n    PREDICT_FILE_PATH = f\"{OUTPUT_RESULTS}{str(i)}\"\n    OUTPUTRESULTS_FILE_PATH = f\"{OUTPUT_RESULTS}{str(i)+'-00000-of-00001'}\"\n    \n    generate_pairs(inf_df[i:i+100], inf_df, SERVING_FILE_PATH)\n\n    csv_to_tfrecord(SERVING_FILE_PATH, TFRECORD_FILE_PATH) \n\n    predictions(TFRECORD_FILE_PATH, PREDICT_FILE_PATH)\n    \n    pred_df = pd.read_csv(OUTPUTRESULTS_FILE_PATH)\n    pred_df = pred_df.sort_values(['id_1', 'id_2', 'match'])\n    rows = pred_df.values\n    output_dict = create_output_dict(rows)\n    write_csv_file(output_dict)\n\n    del pred_df\n    del rows\n    del output_dict\n     \n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-05T19:30:55.629036Z","iopub.execute_input":"2022-07-05T19:30:55.629547Z","iopub.status.idle":"2022-07-05T19:31:30.704149Z","shell.execute_reply.started":"2022-07-05T19:30:55.629502Z","shell.execute_reply":"2022-07-05T19:31:30.703048Z"},"trusted":true},"execution_count":null,"outputs":[]}]}